# Pilot: Reader-Impact — How Editorial Choices Shape Reading
Pilot ID: P3_READER_001
Status: EXPERIMENTAL — NOT FOR PUBLICATION

## Purpose
Observe how different editorial strategies (annotation, retention, glossary reliance, ambiguity visibility) change a reader’s experience. This is descriptive only — not evaluative, not prescriptive.

## Method (simulated)
Use 2–3 short passages (documentary simulations). For each passage, present three versions.

### Passage A (simulated)
1. Minimal intervention
   - Flow: smooth
   - Confidence: low (ambiguity implicit)
   - Meaning: partially clear
   - Emotional reaction: curiosity

2. Moderate intervention
   - Flow: mildly interrupted
   - Confidence: higher
   - Meaning: clearer
   - Emotional reaction: reassurance

3. Heavy intervention
   - Flow: fragmented
   - Confidence: mixed (too many notes)
   - Meaning: overloaded
   - Emotional reaction: frustration

### Passage B (simulated)
1. Minimal intervention
   - Flow: smooth
   - Confidence: uncertain
   - Meaning: vague in key spots
   - Emotional reaction: mild doubt

2. Moderate intervention
   - Flow: slightly interrupted
   - Confidence: improved
   - Meaning: clearer without overload
   - Emotional reaction: trust

3. Heavy intervention
   - Flow: choppy
   - Confidence: reduced (noise)
   - Meaning: harder to track
   - Emotional reaction: fatigue

### Passage C (simulated)
1. Minimal intervention
   - Flow: smooth
   - Confidence: low
   - Meaning: ambiguous
   - Emotional reaction: curiosity

2. Moderate intervention
   - Flow: manageable
   - Confidence: higher
   - Meaning: clarified
   - Emotional reaction: engagement

3. Heavy intervention
   - Flow: fragmented
   - Confidence: mixed
   - Meaning: diluted by commentary
   - Emotional reaction: impatience

## Observations (structured)
- Annotation helps when it clarifies a single ambiguity without adding multiple layers.
- Annotation distracts when notes compete with the primary instruction stream.
- Ambiguity can be useful when it signals historical texture rather than error.
- Clarity can cost nuance when a single term is forced to stand in for multiple meanings.

## Risks (documentary)
- over-annotation crowding out the text
- under-annotation leaving confusion invisible
- glossary dependence slowing reading
- cultural context flattened by simplification

[METHODOLOGY_LOG]
Event: P3_READER_001 reader-impact observation.
Scope: document perceived reader effects without editing or policy.
Method: simulated passages with varying annotation density.
Result: trade-offs documented; no rules created.
[/METHODOLOGY_LOG]

[FAILURE_NOTEBOOK]
CaseID: P3_READER_001_F1
Expected: observe without optimizing.
Observed: temptation to select a “best” annotation level.
Mitigation: recorded contrasts only; no recommendations made.
Lesson: reader impact should be studied without prescribing outcomes.
[/FAILURE_NOTEBOOK]

## Exit Criteria
Reader trade-offs visible; no recommendations issued.

## Rollback
Delete this file → zero system change.

## Consolidation Summary (Phase-3)

What we validated:  
Different editorial strategies measurably change reader experience.
Annotation can clarify, but it can also interrupt. Retention preserves
texture but leaves questions. Glossaries help — and sometimes slow the
reader. Observing these trade-offs is valuable on its own.

What failed safely:  
We did not decide which level of intervention is “best.” No guidance was
issued. All examples remained simulated and reversible.

Governance behavior:  
This pilot stayed descriptive. Human Gate was not involved because no
editorial rules were proposed. Documentation captured the learning
without shaping practice.

Key insight:  
Reader impact is real — but understanding it must come before policy. In
Phase-3, noticing effects is enough.

Status: CONSOLIDATED — learning captured, no guidelines created.
